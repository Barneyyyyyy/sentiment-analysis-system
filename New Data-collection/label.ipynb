{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_file_path = r'C:\\Users\\bhavi\\OneDrive\\Documents\\Conestoga courses\\AI Sem1\\ML\\sentiment-analysis-system\\New Data-collection\\reddit.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotated dataset saved to C:\\Users\\bhavi\\OneDrive\\Documents\\Conestoga courses\\AI Sem1\\ML\\sentiment-analysis-system\\New Data-collection\\Labelled.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the dataset\n",
    "csv_file_path = r'C:\\Users\\bhavi\\OneDrive\\Documents\\Conestoga courses\\AI Sem1\\ML\\sentiment-analysis-system\\New Data-collection\\reddit.csv'\n",
    "df = pd.read_csv(csv_file_path)  # Load the data into a DataFrame\n",
    "\n",
    "# Load the sentiment analysis model\n",
    "classifier = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
    "\n",
    "# Define a function to annotate sentiment\n",
    "def annotate_sentiment(text):\n",
    "    try:\n",
    "        result = classifier(text)\n",
    "        return result[0]['label'].lower()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {e}\")\n",
    "        return 'neutral'\n",
    "\n",
    "# Annotate each post with a sentiment label\n",
    "df['Sentiment'] = df['Title'].apply(annotate_sentiment)\n",
    "\n",
    "# Save the annotated dataset to a new CSV file\n",
    "annotated_file_path = r'C:\\Users\\bhavi\\OneDrive\\Documents\\Conestoga courses\\AI Sem1\\ML\\sentiment-analysis-system\\New Data-collection\\Labelled.csv'\n",
    "df.to_csv(annotated_file_path, index=False)\n",
    "\n",
    "print(f\"Annotated dataset saved to {annotated_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotated dataset saved to C:\\Users\\bhavi\\OneDrive\\Documents\\Conestoga courses\\AI Sem1\\ML\\sentiment-analysis-system\\New Data-collection\\Labelled.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the dataset\n",
    "csv_file_path = r'C:\\Users\\bhavi\\OneDrive\\Documents\\Conestoga courses\\AI Sem1\\ML\\sentiment-analysis-system\\New Data-collection\\reddit.csv'\n",
    "df = pd.read_csv(csv_file_path)  # Load the data into a DataFrame\n",
    "\n",
    "# Load the sentiment analysis model\n",
    "classifier = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
    "\n",
    "# Updated function to annotate sentiment including 'neutral'\n",
    "def annotate_sentiment(text):\n",
    "    try:\n",
    "        # Get the result from the classifier\n",
    "        result = classifier(text)\n",
    "        # Get the label and score\n",
    "        label = result[0]['label'].lower()\n",
    "        score = result[0]['score']\n",
    "\n",
    "        # Set thresholds for determining 'neutral' sentiment\n",
    "        neutral_threshold_upper = 0.6\n",
    "        neutral_threshold_lower = 0.4\n",
    "\n",
    "        # If the score is not confident, label as 'neutral'\n",
    "        if neutral_threshold_lower <= score <= neutral_threshold_upper:\n",
    "            return 'neutral'\n",
    "        else:\n",
    "            return label\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {e}\")\n",
    "        return 'neutral'\n",
    "\n",
    "# Annotate each post with a sentiment label\n",
    "df['Sentiment'] = df['Title'].apply(annotate_sentiment)\n",
    "\n",
    "# Save the annotated dataset to a new CSV file\n",
    "annotated_file_path = r'C:\\Users\\bhavi\\OneDrive\\Documents\\Conestoga courses\\AI Sem1\\ML\\sentiment-analysis-system\\New Data-collection\\Labelled.csv'\n",
    "df.to_csv(annotated_file_path, index=False)\n",
    "\n",
    "print(f\"Annotated dataset saved to {annotated_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment\n",
      "negative    3281\n",
      "positive    2216\n",
      "neutral      152\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the annotated CSV file\n",
    "df = pd.read_csv('C:\\\\Users\\\\bhavi\\\\OneDrive\\\\Documents\\\\Conestoga courses\\\\AI Sem1\\\\ML\\\\sentiment-analysis-system\\\\New Data-collection\\\\Labelled.csv')\n",
    "\n",
    "# Count the number of occurrences of each sentiment label\n",
    "sentiment_counts = df['Sentiment'].value_counts()\n",
    "\n",
    "# Print the count of each sentiment\n",
    "print(sentiment_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bhavi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bhavi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed dataset saved to C:\\Users\\bhavi\\OneDrive\\Documents\\Conestoga courses\\AI Sem1\\ML\\sentiment-analysis-system\\New Data-collection\\Preprocessed_Labelled.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from emoji import demojize\n",
    "import re\n",
    "\n",
    "# NLTK Downloads\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('C:\\\\Users\\\\bhavi\\\\OneDrive\\\\Documents\\\\Conestoga courses\\\\AI Sem1\\\\ML\\\\sentiment-analysis-system\\\\New Data-collection\\\\Labelled.csv')\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Convert emojis to text\n",
    "    text = demojize(text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Lowercase and remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token.lower() for token in tokens if token.lower() not in stop_words]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    # Remove non-alphabetic characters and keep the tokens\n",
    "    tokens = [token for token in tokens if token.isalpha()]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to each title\n",
    "df['Processed_Title'] = df['Title'].apply(preprocess_text)\n",
    "\n",
    "# Define the file path to save the preprocessed dataset\n",
    "preprocessed_file_path = 'C:\\\\Users\\\\bhavi\\\\OneDrive\\\\Documents\\\\Conestoga courses\\\\AI Sem1\\\\ML\\\\sentiment-analysis-system\\\\New Data-collection\\\\Preprocessed_Labelled.csv'\n",
    "\n",
    "# Save the preprocessed dataset\n",
    "df.to_csv(preprocessed_file_path, index=False)\n",
    "\n",
    "print(f\"Preprocessed dataset saved to {preprocessed_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_cpu",
   "language": "python",
   "name": "tensorflow_cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

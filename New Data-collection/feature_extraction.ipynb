{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF features saved to C:\\Users\\bhavi\\OneDrive\\Documents\\Conestoga courses\\AI Sem1\\ML\\sentiment-analysis-system\\New Data-collection\\TFIDF_Features.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy.sparse\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r'C:\\Users\\bhavi\\OneDrive\\Documents\\Conestoga courses\\AI Sem1\\ML\\sentiment-analysis-system\\New Data-collection\\Preprocessed.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Fill NaN values with an empty string\n",
    "df['Processed_Title'].fillna('', inplace=True)\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000,  # Larger number of features given the dataset size\n",
    "                                   ngram_range=(1, 2),  # Unigrams and bigrams\n",
    "                                   min_df=3,  # Lower min_df since dataset isn't very large\n",
    "                                   max_df=0.85,  # Exclude terms that are too common\n",
    "                                   sublinear_tf=True)  # Apply sublinear scaling\n",
    "\n",
    "# Apply the vectorizer to the processed titles\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['Processed_Title'])\n",
    "\n",
    "# Convert the matrix to a DataFrame\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "# Optionally, save the matrix to a CSV file\n",
    "output_file = r'C:\\Users\\bhavi\\OneDrive\\Documents\\Conestoga courses\\AI Sem1\\ML\\sentiment-analysis-system\\New Data-collection\\TFIDF_Features.csv'\n",
    "tfidf_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"TF-IDF features saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load your dataset\n",
    "file_path = r'C:\\Users\\bhavi\\OneDrive\\Documents\\Conestoga courses\\AI Sem1\\ML\\sentiment-analysis-system\\New Data-collection\\Preprocessed.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Fill NaN values with an empty string\n",
    "df['Processed_Title'].fillna('', inplace=True)\n",
    "\n",
    "# Tokenize your preprocessed text\n",
    "tokenized_text = [text.split() for text in df['Processed_Title']]\n",
    "\n",
    "# Create and train the Word2Vec model\n",
    "word2vec_model = Word2Vec(tokenized_text, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Define a function to average word vectors for a text\n",
    "def document_vector(doc):\n",
    "    # remove out-of-vocabulary words\n",
    "    doc = [word for word in doc if word in word2vec_model.wv]\n",
    "    if not doc:\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "    return np.mean(word2vec_model.wv[doc], axis=0)\n",
    "\n",
    "# Apply the function to each document\n",
    "df['Doc_Vector'] = [document_vector(doc) for doc in tokenized_text]\n",
    "\n",
    "# Split the vectors into their own columns for CSV output\n",
    "vector_df = pd.DataFrame(df['Doc_Vector'].tolist())\n",
    "\n",
    "# Concatenate the original dataframe with the vector dataframe\n",
    "df = pd.concat([df, vector_df], axis=1)\n",
    "\n",
    "# Drop the 'Doc_Vector' column\n",
    "df.drop('Doc_Vector', axis=1, inplace=True)\n",
    "\n",
    "# Save the dataframe to a CSV file\n",
    "output_file_path = 'word_vectors.csv'\n",
    "df.to_csv(output_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load the dataset\n",
    "input_file_path = 'C:\\\\Users\\\\bhavi\\\\OneDrive\\\\Documents\\\\Conestoga courses\\\\AI Sem1\\\\ML\\\\sentiment-analysis-system\\\\New Data-collection\\\\Preprocessed.csv'\n",
    "df = pd.read_csv(input_file_path)\n",
    "\n",
    "# Replace NaN values in 'Processed_Title' column with empty string\n",
    "df['Processed_Title'].fillna(\"\", inplace=True)\n",
    "\n",
    "# Initialize BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to encode text using BERT\n",
    "def bert_encode(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True, padding='max_length')\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].detach().numpy()\n",
    "\n",
    "# Apply BERT encoding to each item in 'Processed_Title'\n",
    "bert_embeddings = df['Processed_Title'].apply(bert_encode)\n",
    "\n",
    "# Convert embeddings to a DataFrame\n",
    "bert_embeddings_df = pd.DataFrame([embedding[0] for embedding in bert_embeddings])\n",
    "\n",
    "# Concatenate original dataframe with embeddings\n",
    "final_df = pd.concat([df, bert_embeddings_df], axis=1)\n",
    "\n",
    "# Save the final DataFrame to a new CSV file\n",
    "output_file_path = 'C:\\\\Users\\\\bhavi\\\\OneDrive\\\\Documents\\\\Conestoga courses\\\\AI Sem1\\\\ML\\\\sentiment-analysis-system\\\\New Data-collection\\\\feature_datasets\\\\BERT_Features.csv'\n",
    "final_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"BERT features dataset saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "868067a4da66437b87788db89e747abb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bhavi\\OneDrive\\Documents\\Conestoga courses\\AI Sem1\\FoundationsOfML\\CSCN8010\\venv\\tensorflow_cpu\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\bhavi\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fb268d20e1d42028318bfb0e4dc2c1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8866df81251942e7a2d26bca28b3382d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe1ef57a711f43dc8c9d8045e37db894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0fd79e2ea374f2bb55c918368068c1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load the dataset\n",
    "input_file_path = 'C:\\\\Users\\\\bhavi\\\\OneDrive\\\\Documents\\\\Conestoga courses\\\\AI Sem1\\\\ML\\\\sentiment-analysis-system\\\\New Data-collection\\\\Preprocessed.csv'\n",
    "df = pd.read_csv(input_file_path)\n",
    "\n",
    "# Replace NaN values in 'Processed_Title' column with empty string\n",
    "df['Processed_Title'].fillna(\"\", inplace=True)\n",
    "\n",
    "# Initialize BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to encode text using BERT\n",
    "def bert_encode(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True, padding='max_length')\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].detach().numpy()\n",
    "\n",
    "# Apply BERT encoding to each item in 'Processed_Title'\n",
    "bert_embeddings = df['Processed_Title'].apply(bert_encode)\n",
    "\n",
    "# Convert embeddings to a DataFrame\n",
    "bert_embeddings_df = pd.DataFrame([embedding[0] for embedding in bert_embeddings])\n",
    "\n",
    "# Concatenate original dataframe with embeddings\n",
    "final_df = pd.concat([df, bert_embeddings_df], axis=1)\n",
    "\n",
    "# Save the final DataFrame to a new CSV file\n",
    "output_file_path = 'C:\\\\Users\\\\bhavi\\\\OneDrive\\\\Documents\\\\Conestoga courses\\\\AI Sem1\\\\ML\\\\sentiment-analysis-system\\\\New Data-collection\\\\feature_datasets\\\\BERT_Features.csv'\n",
    "final_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"BERT features dataset saved to {output_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_cpu",
   "language": "python",
   "name": "tensorflow_cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

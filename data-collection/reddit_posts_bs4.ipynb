{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Reddit Posts using BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import time\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the url for the webpage that we want to scrape. we're using [Reddit's old website](https://old.reddit.com) for simplicity. Before we start writing the script, first we take a look to the website and see its structure "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the Page with BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we need to request the web page using the ‘requests’ library.\n",
    "url = \"https://old.reddit.com\"\n",
    "\n",
    "# Headers to simulate a browser visit.\n",
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "# Returns a response object which contains the entire source code of the HTML file.\n",
    "response = requests.get(url, headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the page content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "# Find all the posts in the page using the class name 'thing' which is the class name of the div tag that contains the posts.\n",
    "posts = soup.find_all('div', class_='thing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store the posts\n",
    "posts_data = []\n",
    "\n",
    "while len(posts_data) < 1000: # Get 1000 posts\n",
    "    for post in posts:\n",
    "        # Create a dictionary to store the post data\n",
    "        post_data = {}\n",
    "        # Get the title element of the post\n",
    "        title_element = post.find('p', class_=\"title\")\n",
    "        # Check if the post has a title and add it to the dictionary\n",
    "        post_data['Title'] = title_element.text if title_element else \"N/A\"\n",
    "\n",
    "        author_element = post.find('a', class_='author')\n",
    "        # Check if the post has an author and add it to the dictionary\n",
    "        post_data['Author'] = author_element.text if author_element else \"N/A\"\n",
    "\n",
    "        comments_element = post.find('a', class_='comments')\n",
    "        # Check if the post has comments and get only the number of comments and add it to the dictionary\n",
    "        post_data['Comments'] = comments_element.text.split()[0] if comments_element else 0\n",
    "\n",
    "        likes_element = post.find(\"div\", attrs={\"class\": \"score likes\"})\n",
    "        # Check if the post has likes and add it to the dictionary\n",
    "        if likes_element and likes_element.text == \"•\":\n",
    "            post_data['Likes'] = 0\n",
    "        elif likes_element:\n",
    "            post_data['Likes'] = likes_element.text\n",
    "        else:\n",
    "            post_data['Likes'] = \"N/A\"\n",
    "\n",
    "        # Add the post data to the list\n",
    "        posts_data.append(post_data)\n",
    "\n",
    "    # Get the button that takes us to the next page\n",
    "    next_button = soup.find('span', class_='next-button')\n",
    "    # Get the url of the next page\n",
    "    url = next_button.find('a').get('href') if next_button else None\n",
    "\n",
    "    if not url:\n",
    "        break\n",
    "\n",
    "    time.sleep(2)  # Wait for 2 seconds before making the next request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1026\n"
     ]
    }
   ],
   "source": [
    "# Write the posts data to a CSV file\n",
    "with open('reddit_posts_bs4.csv', 'w', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=['Title', 'Author', 'Likes', 'Comments'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(posts_data)\n",
    "\n",
    "print(len(posts_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Henry Kissinger, secretary of state to Richard...</td>\n",
       "      <td>MrRedXiii</td>\n",
       "      <td>31.5k</td>\n",
       "      <td>2930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Space Karen during interview yesterday where h...</td>\n",
       "      <td>ohnoh18</td>\n",
       "      <td>3697</td>\n",
       "      <td>991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One of the best plot twists you’ll read (v.red...</td>\n",
       "      <td>Remarkable_Toe2603</td>\n",
       "      <td>12.0k</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My boss wanted me to make an infographic. How’...</td>\n",
       "      <td>RedditforBusiness</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What something that’s completely normal in mov...</td>\n",
       "      <td>covalentcookies</td>\n",
       "      <td>925</td>\n",
       "      <td>1647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title              Author   \n",
       "0  Henry Kissinger, secretary of state to Richard...           MrRedXiii  \\\n",
       "1  Space Karen during interview yesterday where h...             ohnoh18   \n",
       "2  One of the best plot twists you’ll read (v.red...  Remarkable_Toe2603   \n",
       "3  My boss wanted me to make an infographic. How’...   RedditforBusiness   \n",
       "4  What something that’s completely normal in mov...     covalentcookies   \n",
       "\n",
       "   Likes  Comments  \n",
       "0  31.5k      2930  \n",
       "1   3697       991  \n",
       "2  12.0k       333  \n",
       "3      0         0  \n",
       "4    925      1647  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_posts = pd.read_csv('reddit_posts_bs4.csv')\n",
    "reddit_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The post is neutral\n"
     ]
    }
   ],
   "source": [
    "# We are using transformers library to get the sentiment of the posts.\n",
    "from transformers import pipeline\n",
    "\n",
    "# Create a sentiment analysis pipeline\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "\n",
    "results = classifier(\"We hope you don't hate it.\")\n",
    "\n",
    "score = results[0]['score']\n",
    "if 0.4 <= score <= 0.6:\n",
    "    print(\"The post is neutral\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive posts: 114\n",
      "Negative posts: 912\n",
      "Neutral posts: 0\n"
     ]
    }
   ],
   "source": [
    "positive_posts = []\n",
    "negative_posts = []\n",
    "neutral_posts = []\n",
    "\n",
    "results = classifier(reddit_posts['Title'].tolist())\n",
    "\n",
    "for post, result in zip(reddit_posts['Title'].tolist(), results):\n",
    "    score = round(result['score'], 4)\n",
    "    if 0.4 <= score <= 0.6:\n",
    "        label = 'NEUTRAL'\n",
    "        neutral_posts.append((post, score))\n",
    "    elif result['label'] == 'POSITIVE':\n",
    "        label = 'POSITIVE'\n",
    "        positive_posts.append((post, score))\n",
    "    else:\n",
    "        label = 'NEGATIVE'\n",
    "        negative_posts.append((post, score))\n",
    "\n",
    "print(f\"Positive posts: {len(positive_posts)}\")\n",
    "print(f\"Negative posts: {len(negative_posts)}\")\n",
    "print(f\"Neutral posts: {len(neutral_posts)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

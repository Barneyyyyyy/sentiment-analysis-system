{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotated dataset saved to C:\\Users\\bhavi\\OneDrive\\Documents\\Conestoga courses\\AI Sem1\\ML\\sentiment-analysis-system\\New Data-collection\\Labelled.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the dataset\n",
    "csv_file_path = r'C:\\Users\\bhavi\\OneDrive\\Documents\\Conestoga courses\\AI Sem1\\ML\\sentiment-analysis-system\\New Data-collection\\reddit.csv'\n",
    "df = pd.read_csv(csv_file_path)  # Load the data into a DataFrame\n",
    "\n",
    "# Load the sentiment analysis model\n",
    "classifier = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
    "\n",
    "# Define a function to annotate sentiment\n",
    "def annotate_sentiment(text):\n",
    "    try:\n",
    "        result = classifier(text)\n",
    "        return result[0]['label'].lower()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {e}\")\n",
    "        return 'neutral'\n",
    "\n",
    "# Annotate each post with a sentiment label\n",
    "df['Sentiment'] = df['Title'].apply(annotate_sentiment)\n",
    "\n",
    "# Save the annotated dataset to a new CSV file\n",
    "annotated_file_path = r'C:\\Users\\bhavi\\OneDrive\\Documents\\Conestoga courses\\AI Sem1\\ML\\sentiment-analysis-system\\New Data-collection\\Labelled.csv'\n",
    "df.to_csv(annotated_file_path, index=False)\n",
    "\n",
    "print(f\"Annotated dataset saved to {annotated_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotated dataset saved to C:\\Users\\bhavi\\OneDrive\\Documents\\Conestoga courses\\AI Sem1\\ML\\sentiment-analysis-system\\New Data-collection\\Labelled.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the dataset\n",
    "csv_file_path = r'C:\\Users\\bhavi\\OneDrive\\Documents\\Conestoga courses\\AI Sem1\\ML\\sentiment-analysis-system\\New Data-collection\\reddit.csv'\n",
    "df = pd.read_csv(csv_file_path)  # Load the data into a DataFrame\n",
    "\n",
    "# Load the sentiment analysis model\n",
    "classifier = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
    "\n",
    "# Updated function to annotate sentiment including 'neutral'\n",
    "def annotate_sentiment(text):\n",
    "    try:\n",
    "        # Get the result from the classifier\n",
    "        result = classifier(text)\n",
    "        # Get the label and score\n",
    "        label = result[0]['label'].lower()\n",
    "        score = result[0]['score']\n",
    "\n",
    "        # Set thresholds for determining 'neutral' sentiment\n",
    "        neutral_threshold_upper = 0.6\n",
    "        neutral_threshold_lower = 0.4\n",
    "\n",
    "        # If the score is not confident, label as 'neutral'\n",
    "        if neutral_threshold_lower <= score <= neutral_threshold_upper:\n",
    "            return 'neutral'\n",
    "        else:\n",
    "            return label\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {e}\")\n",
    "        return 'neutral'\n",
    "\n",
    "# Annotate each post with a sentiment label\n",
    "df['Sentiment'] = df['Title'].apply(annotate_sentiment)\n",
    "\n",
    "# Save the annotated dataset to a new CSV file\n",
    "annotated_file_path = r'C:\\Users\\bhavi\\OneDrive\\Documents\\Conestoga courses\\AI Sem1\\ML\\sentiment-analysis-system\\New Data-collection\\Labelled.csv'\n",
    "df.to_csv(annotated_file_path, index=False)\n",
    "\n",
    "print(f\"Annotated dataset saved to {annotated_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment\n",
      "negative    3281\n",
      "positive    2216\n",
      "neutral      152\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the annotated CSV file\n",
    "df = pd.read_csv('C:\\\\Users\\\\bhavi\\\\OneDrive\\\\Documents\\\\Conestoga courses\\\\AI Sem1\\\\ML\\\\sentiment-analysis-system\\\\New Data-collection\\\\Labelled.csv')\n",
    "\n",
    "# Count the number of occurrences of each sentiment label\n",
    "sentiment_counts = df['Sentiment'].value_counts()\n",
    "\n",
    "# Print the count of each sentiment\n",
    "print(sentiment_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bhavi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bhavi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\bhavi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed dataset saved to C:\\Users\\bhavi\\OneDrive\\Documents\\Conestoga courses\\AI Sem1\\ML\\sentiment-analysis-system\\New Data-collection\\Preprocessed.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from emoji import demojize\n",
    "import re\n",
    "\n",
    "# NLTK Downloads\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('C:\\\\Users\\\\bhavi\\\\OneDrive\\\\Documents\\\\Conestoga courses\\\\AI Sem1\\\\ML\\\\sentiment-analysis-system\\\\New Data-collection\\\\Labelled.csv')\n",
    "\n",
    "# Slang dictionary\n",
    "slang_dict = {\n",
    "    'btw': 'by the way',\n",
    "    'lol': 'laughing out loud',\n",
    "    'idk': 'I do not know',\n",
    "    'imo': 'in my opinion',\n",
    "    'imho': 'in my humble opinion',\n",
    "    'brb': 'be right back',\n",
    "    'tbh': 'to be honest',\n",
    "    'lmao': 'laughing my ass off',\n",
    "    'rofl': 'rolling on the floor laughing',\n",
    "    'smh': 'shaking my head',\n",
    "    'omg': 'oh my god',\n",
    "    'ttyl': 'talk to you later',\n",
    "    'afaik': 'as far as I know',\n",
    "    'irl': 'in real life',\n",
    "    'thx': 'thanks',\n",
    "    'pls': 'please',\n",
    "    'dm': 'direct message',\n",
    "    'fyi': 'for your information',\n",
    "    'b4': 'before',\n",
    "    'gr8': 'great',\n",
    "    'u': 'you',\n",
    "    'r': 'are',\n",
    "    'yolo': 'you only live once',\n",
    "    'np': 'no problem',\n",
    "    'g2g': 'got to go',\n",
    "    'tldr': 'too long, didnâ€™t read',\n",
    "    'jk': 'just kidding',\n",
    "    'bff': 'best friends forever',\n",
    "    'icymi': 'in case you missed it',\n",
    "    'fomo': 'fear of missing out',\n",
    "    'ftw': 'for the win',\n",
    "    'wtf': 'what the f***',\n",
    "    'nsfw': 'not safe for work',\n",
    "    'nbd': 'no big deal',\n",
    "    'faq': 'frequently asked questions',\n",
    "    'afk': 'away from keyboard',\n",
    "    'asap': 'as soon as possible'\n",
    "}\n",
    "\n",
    "# Function to extract and process hashtags\n",
    "def process_hashtags(text):\n",
    "    hashtags = re.findall(r'#\\w+', text)\n",
    "    return ' '.join(hashtags)\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Convert emojis to text and process hashtags\n",
    "    text = demojize(text) + ' ' + process_hashtags(text)\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords and apply lemmatization\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "    # Translate slang\n",
    "    tokens = [slang_dict.get(token, token) for token in tokens]\n",
    "\n",
    "    # Remove non-alphabetic characters and keep the tokens\n",
    "    tokens = [token for token in tokens if token.isalpha()]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to each title\n",
    "df['Processed_Title'] = df['Title'].apply(preprocess_text)\n",
    "\n",
    "# Define the file path to save the preprocessed dataset\n",
    "preprocessed_file_path = 'C:\\\\Users\\\\bhavi\\\\OneDrive\\\\Documents\\\\Conestoga courses\\\\AI Sem1\\\\ML\\\\sentiment-analysis-system\\\\New Data-collection\\\\Preprocessed.csv'\n",
    "\n",
    "# Save the preprocessed dataset\n",
    "df.to_csv(preprocessed_file_path, index=False)\n",
    "\n",
    "print(f\"Preprocessed dataset saved to {preprocessed_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_cpu",
   "language": "python",
   "name": "tensorflow_cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
